{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8260711,"sourceType":"datasetVersion","datasetId":4902958},{"sourceId":8260917,"sourceType":"datasetVersion","datasetId":4903103}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T03:07:34.101340Z","iopub.execute_input":"2024-05-14T03:07:34.102073Z","iopub.status.idle":"2024-05-14T03:07:34.993464Z","shell.execute_reply.started":"2024-05-14T03:07:34.102045Z","shell.execute_reply":"2024-05-14T03:07:34.992195Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/d/methylbenzel/vlspdata/vlsp.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# install librari","metadata":{}},{"cell_type":"code","source":"!pip install \"transformers==4.35\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" \"tiktoken\"\n     ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:07:34.995950Z","iopub.execute_input":"2024-05-14T03:07:34.996832Z","iopub.status.idle":"2024-05-14T03:08:06.708145Z","shell.execute_reply.started":"2024-05-14T03:07:34.996786Z","shell.execute_reply":"2024-05-14T03:08:06.707250Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting transformers==4.35\n  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting datasets==2.13.0\n  Downloading datasets-2.13.0-py3-none-any.whl.metadata (20 kB)\nCollecting peft==0.4.0\n  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\nCollecting accelerate==0.21.0\n  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\nCollecting bitsandbytes==0.40.2\n  Downloading bitsandbytes-0.40.2-py3-none-any.whl.metadata (9.8 kB)\nCollecting trl==0.4.7\n  Downloading trl-0.4.7-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (0.4.3)\nCollecting tiktoken\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.35) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.35) (2.31.0)\nCollecting tokenizers<0.15,>=0.14 (from transformers==4.35)\n  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.35) (4.66.1)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0) (15.0.2)\nCollecting dill<0.3.7,>=0.3.0 (from datasets==2.13.0)\n  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0) (0.70.16)\nRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.13.0) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (2.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.35) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.35) (2024.2.2)\nCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35)\n  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.13.0)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.13.0) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\nDownloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.13.0-py3-none-any.whl (485 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.4.7-py3-none-any.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, dill, tiktoken, multiprocess, huggingface-hub, tokenizers, accelerate, transformers, datasets, trl, peft\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.22.2\n    Uninstalling huggingface-hub-0.22.2:\n      Successfully uninstalled huggingface-hub-0.22.2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.29.3\n    Uninstalling accelerate-0.29.3:\n      Successfully uninstalled accelerate-0.29.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.18.0\n    Uninstalling datasets-2.18.0:\n      Successfully uninstalled datasets-2.18.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.6 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.14 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.2 datasets-2.13.0 dill-0.3.6 huggingface-hub-0.17.3 multiprocess-0.70.14 peft-0.4.0 tiktoken-0.7.0 tokenizers-0.14.1 transformers-4.35.0 trl-0.4.7\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom random import randrange\nfrom peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\nfrom trl import SFTTrainer\n     ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:08:06.709542Z","iopub.execute_input":"2024-05-14T03:08:06.709880Z","iopub.status.idle":"2024-05-14T03:08:23.994639Z","shell.execute_reply.started":"2024-05-14T03:08:06.709851Z","shell.execute_reply":"2024-05-14T03:08:23.993733Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-14 03:08:15.345513: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-14 03:08:15.345634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-14 03:08:15.483467: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:08:31.981882Z","iopub.execute_input":"2024-05-14T03:08:31.982500Z","iopub.status.idle":"2024-05-14T03:08:32.012163Z","shell.execute_reply.started":"2024-05-14T03:08:31.982471Z","shell.execute_reply":"2024-05-14T03:08:32.011220Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec017a01fad4e9c91a7a9c11d6178d8"}},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/d/methylbenzel/vlspdata/vlsp.csv\")\ndf = df[:1000]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Concatenate 'Question' and 'Answer' columns into a new column 'text'\ndf['text'] = 'question:\\n' + df['question'] + '\\n\\nanswer:\\n' + df['answer']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(df['text'].loc[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#drop columns other than 'text'\ndf.drop(columns=['question','answer'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# #drop columns other than 'text'\ndf.drop(columns=['question','answer'], axis=1, inplace=True)\ndf = df[:","metadata":{}},{"cell_type":"code","source":"train = Dataset.from_pandas(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# finetune\n","metadata":{}},{"cell_type":"code","source":"\nmodel_id = \"genaitraining/llama-2-7b-domain-tuned\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n#huggingface_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\n!mkdir -p ~/.huggingface\n!echo -n $huggingface_token > ~/.huggingface/token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets verify\n!huggingface-cli whoami","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the type\ncompute_dtype = getattr(torch, \"float16\")\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\n%%time\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pip install --upgrade peft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install git+https://github.com/huggingface/peft","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:14:07.998312Z","iopub.execute_input":"2024-05-14T03:14:07.998917Z","iopub.status.idle":"2024-05-14T03:14:38.070932Z","shell.execute_reply.started":"2024-05-14T03:14:07.998885Z","shell.execute_reply":"2024-05-14T03:14:38.069755Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/peft\n  Cloning https://github.com/huggingface/peft to /tmp/pip-req-build-0bk5_mew\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft /tmp/pip-req-build-0bk5_mew\n  Resolved https://github.com/huggingface/peft to commit 2558dd872dbacd29f6b60668bfee54b594bcbd7f\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (4.35.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (0.21.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.10.1.dev0) (0.17.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.1.dev0) (3.13.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.1.dev0) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.1.dev0) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.1.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.10.1.dev0) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.1.dev0) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.10.1.dev0) (2023.12.25)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.10.1.dev0) (0.14.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.10.1.dev0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.1.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.1.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.1.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.1.dev0) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.10.1.dev0) (1.3.0)\nBuilding wheels for collected packages: peft\n  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for peft: filename=peft-0.10.1.dev0-py3-none-any.whl size=249079 sha256=e324d51c8df08cccd769a2acf1b4a0c12f07b8a0041ba3f4d28a6de355e93828\n  Stored in directory: /tmp/pip-ephem-wheel-cache-xsc3ze2o/wheels/4c/16/67/1002a2d4daa822eff130e6d85b90051b75d2ce0d26b9448e4a\nSuccessfully built peft\nInstalling collected packages: peft\n  Attempting uninstall: peft\n    Found existing installation: peft 0.4.0\n    Uninstalling peft-0.4.0:\n      Successfully uninstalled peft-0.4.0\nSuccessfully installed peft-0.10.1.dev0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n%%time\n# Load the pretrained model\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n                                             quantization_config=bnb_config,\n                                             device_map=\"auto\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# LoRA config based on QLoRA paper\npeft_config = LoraConfig(\n                          lora_alpha=16,\n                          lora_dropout=0.1,\n                          r=64,\n                          bias=\"none\",\n                          task_type=\"CAUSAL_LM\"\n                        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the training arguments. For full list of arguments, check\n#https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\nargs = TrainingArguments(\n    output_dir='llama2-7b-tuned-qna',\n    num_train_epochs=1, # adjust based on the data size\n    per_device_train_batch_size=2, # use 4 if you have more GPU RAM\n    save_strategy=\"epoch\", #steps\n    # evaluation_strategy=\"epoch\",\n    learning_rate=2e-4,\n    fp16=True,\n    seed=42\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create the trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train,\n    # eval_dataset=test,\n    dataset_text_field='text',\n    peft_config=peft_config,\n    max_seq_length=1042,\n    tokenizer=tokenizer,\n    args=args,\n    packing=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# train\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# save model in local\ntrainer.save_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Empty VRAM\ndel model\ndel trainer\nimport gc\ngc.collect()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom peft import AutoPeftModelForCausalLM\n\nnew_model = AutoPeftModelForCausalLM.from_pretrained(\n    'llama2-7b-tuned-qna',\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n).cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('hello')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = \"\"\"\nDựa vào thông tin và trả lời các câu hỏi:\n Trận đánh gây được tiếng vang lúc bấy giờ là trận phục kích của quân ta tại Cầu Giấy ngày 21-12-1873.\nThừa lúc Gác-ni-ê đem quân xuống đánh Nam Định, việc canh phòng Hà Nội sơ hở, quân ta do Hoàng Tá Viêm chỉ huy (có sự phối hợp với đội quân Cờ đen của Lưu Vĩnh Phúc) từ Sơn Tây kéo về Hà Nội, hình thành trận tuyến bao vây quân địch. Nghe tin đó, Gác-ni-ê phải tức tốc đưa quân vào sát thành Hà Nội khiêu chiến. Gác-ni-ê de quân đuổi theo. Rơi vào ổ phục kích của quân ta tại khu vực Cầu Giấy, toán quân Pháp, trong đó có Gác-ni-ê, đã bị tiêu diệt.\nChiến thắng Cầu Giấy lần thứ nhất khiến cho nhân dân ta vô cùng phấn khởi; ngược lại, làm cho thực dân Pháp hoang mang, lo sợ và tìm cách thương lương. Triều đình lại kí Hiệp ước năm 1874 (Hiệp ước Giáp Tuất), theo đó quân Pháp rút khỏi Hà Nội và các tỉnh đồng bằng Bắc Kì, nhưng vẫn có điều kiện tiếp tục xây dựng cơ sở để thực hiện các bước xâm lược về sau.\n\"\"\"\n#ground truth = \"The output format of the tasks that the Florence-2 model can handle is text forms, whether it be captioning, object detection, grounding or segmentation.\"\n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new_model.device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noutputs = new_model.generate(input_ids=input_ids,\n                         max_new_tokens=200,\n                        #  do_sample=True,\n                        #  top_p=0.9,\n                         temperature=0.6)\n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Merge LoRA and base model\nmerged_model = new_model.merge_and_unload()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the merged model\nmerged_model.save_pretrained(\"metallama2-7b-qa-tuned-merged\", safe_serialization=True)\ntokenizer.save_pretrained(\"metallama2-7b-qa-tuned-merged\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# push merged model to the hub\n\nhf_model_repo = \"Katalan/VlspQA-llama2\"\nnew_model.push_to_hub(hf_model_repo)\ntokenizer.push_to_hub(hf_model_repo)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig\n\n# Get the type\ncompute_dtype = getattr(torch, \"float16\")\n\n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:08:53.259017Z","iopub.execute_input":"2024-05-14T03:08:53.259861Z","iopub.status.idle":"2024-05-14T03:08:53.266361Z","shell.execute_reply.started":"2024-05-14T03:08:53.259828Z","shell.execute_reply":"2024-05-14T03:08:53.265449Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nhf_model_repo = \"Katalan/VlspQA-llama2\"\n\n# Get the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(hf_model_repo)\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(hf_model_repo,\n                                             quantization_config=bnb_config,\n                                             device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:14:51.370837Z","iopub.execute_input":"2024-05-14T03:14:51.371217Z","iopub.status.idle":"2024-05-14T03:14:59.069826Z","shell.execute_reply.started":"2024-05-14T03:14:51.371188Z","shell.execute_reply":"2024-05-14T03:14:59.068544Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c680ccebc735432db8f803c896c9a9f8"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(hf_model_repo)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhf_model_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3553\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3550\u001b[0m     model \u001b[38;5;241m=\u001b[39m quantizer\u001b[38;5;241m.\u001b[39mpost_init_model(model)\n\u001b[1;32m   3552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3553\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3554\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3555\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3557\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   3561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:143\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapter_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     adapter_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, inject_adapter_in_model, load_peft_weights\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_peft_model_state_dict\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_peft_config_loaded \u001b[38;5;129;01mand\u001b[39;00m adapter_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config:\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'inject_adapter_in_model' from 'peft' (/opt/conda/lib/python3.10/site-packages/peft/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'inject_adapter_in_model' from 'peft' (/opt/conda/lib/python3.10/site-packages/peft/__init__.py)","output_type":"error"}]},{"cell_type":"code","source":"prompt = \"\"\"\nTrong một quần xã rừng tự nhiên ở vùng Đông Nam Á, các loài động vật ăn có cỡ lớn như bò rừng mỗi khi di chuyển thường đánh động và làm các loài côn trùng bay khỏi tổ. Lúc này, các loài chim như diệc bạc sẽ bắt các côn trùng bay khỏi tổ làm thức ăn. Việc côn trùng bay khỏi tổ cũng như việc chim diệc bạc bắt côn trùng không ảnh hưởng gì đến đời sống bò rừng. Chim gõ bò có thể bắt ve bét trên da bò rừng làm thức ăn. \nMối quan hệ giữa diệc bạc và côn trùng dẫn đến hiện tượng gì?\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Generate response\n%%time\ninput_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids\noutputs = model.generate(input_ids=input_ids,\n                         max_new_tokens=200,\n                         temperature=0.6)\n\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n\n# Print the result\nprint(f\"Generated response:\\n{result}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport transformers\n\ntokenizer = AutoTokenizer.from_pretrained(\"Katalan/VlspQA-llama2\",  trust_remote_code=True)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=\"Katalan/VlspQA-llama2\",\n    trust_remote_code=True\n\n)\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n%%time\nsequences = pipeline(\n    prompt,\n    temperature=0.6,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor seq in sequences:\n    print(seq['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import HfApi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"api = HfApi()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# api.upload_folder(\n#     folder_path=\"/kaggle/working/llama2-7b-tuned-qna/checkpoint-500\",\n#     path_in_repo = \".\",\n#     repo_id=\"TrinhDacPhu/questionansweringllma2\",\n#     repo_type=\"model\"\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}